import argparse
import json
import os
import sys
import csv
from datetime import datetime
from collections import Counter

from src.core.file_finder import find_files
from src.core.file_processor import create_zip_package

def setup_find_parser(subparsers):
    parser = subparsers.add_parser("find", help="Find, classify, and log SUNAT files.")
    parser.add_argument("path", type=str, help="Absolute directory path to search in.")
    parser.set_defaults(func=run_find)

def setup_process_parser(subparsers):
    parser = subparsers.add_parser("process", help="Process a scan log to create a consolidated ZIP.")
    parser.add_argument("input_csv", type=str, help="Path to the CSV log file generated by the 'find' command.")
    parser.add_argument("output_dir", type=str, help="Absolute directory path to save the output ZIP file.")
    parser.add_argument("--delete-originals", action="store_true", help="Delete ALL original container files listed in the log.")
    parser.set_defaults(func=run_process)

def run_find(args):
    if not os.path.isabs(args.path):
        print(f"Error: The provided path '{args.path}' is not an absolute path.")
        return
    if not os.path.isdir(args.path):
        print(f"Error: The path '{args.path}' does not exist or is not a directory.")
        return

    print(f"Buscando archivos SUNAT en: {args.path}...\n")
    found_files = find_files(args.path)

    if not found_files:
        print("No se encontraron archivos.")
        return

    # --- Statistics ---
    total_files = len(found_files)
    classifications = [f.get('classification', 'unknown') for f in found_files]
    stats = Counter(classifications)

    print("--- Resumen del Escaneo ---")
    print(f"Total de archivos encontrados: {total_files}")
    print("\nClasificación de archivos:")
    for classification, count in stats.items():
        print(f"- {classification}: {count}")
    print("---------------------------\n")

    # --- CSV Log File ---
    log_dir = 'logs'
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"find_{timestamp}.csv"
    log_filepath = os.path.join(log_dir, log_filename)

    try:
        with open(log_filepath, 'w', newline='', encoding='utf-8') as csvfile:
            # Define headers - use a subset of all possible keys
            headers = ["status", "classification", "filename", "path", "ext"]
            writer = csv.DictWriter(csvfile, fieldnames=headers, extrasaction='ignore')
            
            writer.writeheader()
            writer.writerows(found_files)
        
        print(f"Resultados guardados en: {log_filepath}")
    except IOError as e:
        print(f"Error al guardar el archivo de log: {e}")

def run_process(args):
    if not os.path.exists(args.input_csv):
        print(f"Error: El archivo de log no existe: {args.input_csv}")
        return
    if not os.path.isabs(args.output_dir):
        print(f"Error: El directorio de salida '{args.output_dir}' no es una ruta absoluta.")
        return

    print(f"Iniciando procesamiento del log: {args.input_csv}")

    # --- Read and prepare file lists ---
    files_to_package = []
    physical_files_to_delete = set()
    try:
        with open(args.input_csv, 'r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                if row.get('status') == 'UNICO':
                    files_to_package.append(row)
                
                base_path = row['path'].split(':')[0]
                physical_files_to_delete.add(base_path)

    except (IOError, csv.Error) as e:
        print(f"Error leyendo el archivo CSV: {e}")
        return

    if not files_to_package:
        print("No se encontraron archivos 'UNICO' en el log para procesar.")
        return

    # --- Statistics for Processing ---
    total_to_package = len(files_to_package)
    classifications = [f.get('classification', 'unknown') for f in files_to_package]
    stats = Counter(classifications)

    print("--- Resumen del Procesamiento ---")
    print(f"Total de archivos únicos a empaquetar: {total_to_package}")
    print("\nClasificación de archivos a empaquetar:")
    for classification, count in stats.items():
        print(f"- {classification}: {count}")
    print("-----------------------------------\n")

    # --- Generate Filenames and Paths ---
    log_dir = 'logs'
    os.makedirs(args.output_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_filename = f"consolidado_{timestamp}.zip"
    output_zip_path = os.path.join(args.output_dir, zip_filename)
    process_log_filename = f"process_{timestamp}.txt"
    process_log_filepath = os.path.join(log_dir, process_log_filename)

    # --- Determine which files to delete ---
    files_for_deletion = physical_files_to_delete if args.delete_originals else set()
    if args.delete_originals:
        print(f"Se eliminarán {len(files_for_deletion)} archivos físicos originales.")

    create_zip_package(files_to_package, files_for_deletion, output_zip_path, log_filepath=process_log_filepath)


def run_cli():
    # A new top-level parser
    parser = argparse.ArgumentParser(
        description="A tool to find and process SUNAT files."
    )
    subparsers = parser.add_subparsers(title="Commands", dest="command", required=True)

    # Setup subparsers for each command
    setup_find_parser(subparsers)
    setup_process_parser(subparsers)

    # Parse args and call the appropriate function
    args = parser.parse_args()
    args.func(args)